"""
ìœ ì‚¬ë„ ë¶„ì„ ëª¨ë“ˆ
================

ì´ ëª¨ë“ˆì€ ì‚¬ìš©ì ìš”ì²­ê³¼ í”„ë¡œì íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³ 
ì í•©í•œ ì¸ì›ì„ ì¶”ì²œí•˜ê¸° ìœ„í•œ í•µì‹¬ ë¡œì§ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (OpenAI Embeddings ì‚¬ìš©)
- í”„ë¡œì íŠ¸ë³„ ì í•©ë„ ì ìˆ˜ ì‚°ì¶œ
- ì‹œê°„ ê°€ì¤‘ì¹˜ ì ìš© (ìµœê·¼ í”„ë¡œì íŠ¸ ìš°ëŒ€)
- ìƒì„¸í•œ ì¶”ì²œ ì´ìœ  ìƒì„±

ì‘ì„±ì: AI Assistant
ë²„ì „: 1.1.0
"""

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any, Tuple
from datetime import datetime
import re
import os
import pickle
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from .config import VECTOR_DB_DIR, VECTOR_COLLECTION_NAME, PROJECT_DATA_PATH
from .data_loader import ProjectDataLoader
from .config import (
    SIMILARITY_THRESHOLD, 
    RECENT_PROJECT_WEIGHT, 
    TAG_WEIGHT, 
    OVERVIEW_WEIGHT, 
    TITLE_WEIGHT
)

class SimilarityAnalyzer:
    """
    í”„ë¡œì íŠ¸ì™€ ì‚¬ìš©ì ìš”ì²­ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” í•œêµ­ì–´ íŠ¹í™” ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    í…ìŠ¤íŠ¸ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ë‹¤ì–‘í•œ ìš”ì†Œë¥¼
    ì¢…í•©í•˜ì—¬ ìµœì¢… ì í•©ë„ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        """
        ìœ ì‚¬ë„ ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ `OPENAI_API_KEY`ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        """
        self._backend = "openai"
        self._openai_embeddings = None
        self._st_model = None
        self._vector_store = None
        self._metadata_store = None

        # OpenAI embeddingsë§Œ ì‚¬ìš© (í´ë°± ì œê±°)
        try:
            # text-embedding-3-small: 1536-dim, ë¹ ë¥´ê³  ì €ë ´í•¨
            self._openai_embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
            self._backend = "openai"
        except Exception as openai_error:
            # OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ API í˜¸ì¶œ ì‹¤íŒ¨
            raise RuntimeError(
                f"OpenAI ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {str(openai_error)}\n"
                "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ê³  ìœ íš¨í•œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            ) from openai_error

        # FAISS VectorDB ì´ˆê¸°í™” (ì¡´ì¬í•˜ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ì§€ì—° ìƒì„±)
        try:
            self._load_faiss_index()
        except Exception:
            self._vector_store = None
            self._metadata_store = None

    # ---------------------------------------------------------------------
    # FAISS ì¸ë±ìŠ¤ ê´€ë¦¬
    # ---------------------------------------------------------------------
    def _load_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        faiss_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.faiss")
        metadata_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.pkl")
        
        if os.path.exists(faiss_path) and os.path.exists(metadata_path):
            try:
                self._vector_store = FAISS.load_local(
                    VECTOR_DB_DIR, 
                    self._openai_embeddings,
                    allow_dangerous_deserialization=True
                )
                with open(metadata_path, 'rb') as f:
                    self._metadata_store = pickle.load(f)
                print(f"âœ… FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {faiss_path}")
            except Exception as e:
                print(f"âš ï¸ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._vector_store = None
                self._metadata_store = None
        else:
            self._vector_store = None
            self._metadata_store = None

    def _save_faiss_index(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        if self._vector_store is not None and self._metadata_store is not None:
            try:
                # ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(VECTOR_DB_DIR, exist_ok=True)
                
                # FAISS ì¸ë±ìŠ¤ ì €ì¥
                self._vector_store.save_local(VECTOR_DB_DIR)
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                metadata_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.pkl")
                with open(metadata_path, 'wb') as f:
                    pickle.dump(self._metadata_store, f)
                
                print(f"âœ… FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {VECTOR_DB_DIR}")
            except Exception as e:
                print(f"âŒ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ---------------------------------------------------------------------
    # ë²¡í„°DB ìƒ‰ì¸/ë™ê¸°í™”
    # ---------------------------------------------------------------------
    def sync_vector_db(self) -> int:
        """
        í”„ë¡œì íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì—¬ FAISS VectorDBì— ì„ë² ë”©/ì €ì¥í•©ë‹ˆë‹¤.
        ê¸°ì¡´ ì¸ë±ìŠ¤ëŠ” ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        Returns: ì €ì¥ëœ ë¬¸ì„œ ìˆ˜
        """
        data_loader = ProjectDataLoader()
        projects = data_loader.get_all_projects()
        if not projects:
            return 0

        if self._openai_embeddings is None:
            raise RuntimeError("OpenAI ì„ë² ë”©ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ê¸°ì¡´ ì¸ë±ìŠ¤ íŒŒì¼ ì‚­ì œ
        faiss_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.faiss")
        metadata_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.pkl")
        
        for path in [faiss_path, metadata_path]:
            if os.path.exists(path):
                try:
                    os.remove(path)
                except Exception:
                    pass

        texts: List[str] = []
        metadatas: List[Dict[str, Any]] = []

        for idx, project in enumerate(projects):
            title = project.get('í”„ë¡œì íŠ¸ëª…', '')
            overview = project.get('í”„ë¡œì íŠ¸ê°œìš”', '')
            tags = project.get('í”„ë¡œì íŠ¸íƒœê·¸', [])
            joined = f"[ì œëª©]{title}\n[ê°œìš”]{overview}\n[íƒœê·¸]{', '.join(tags)}"
            texts.append(joined)
            metadatas.append({
                "project_name": title,
                "period": project.get('í”„ë¡œì íŠ¸ê¸°ê°„', ''),
                "tags": project.get('í”„ë¡œì íŠ¸íƒœê·¸', []),
                "participants": project.get('ì°¸ì—¬ìëª…ë‹¨', []),
                "project_id": f"project_{idx}"
            })

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        try:
            self._vector_store = FAISS.from_texts(
                texts=texts,
                embedding=self._openai_embeddings,
                metadatas=metadatas
            )
            self._metadata_store = metadatas
            
            # ì¸ë±ìŠ¤ ì €ì¥
            self._save_faiss_index()
            
            return len(texts)
        except Exception as e:
            print(f"âŒ FAISS ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return 0

    def search_similar_projects(self, query: str, k: int = 100) -> List[Tuple[Dict[str, Any], float]]:
        """
        FAISS VectorDBì—ì„œ ì§ˆì˜ì™€ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ë¥¼ ìƒìœ„ kê°œ ë°˜í™˜í•©ë‹ˆë‹¤.
        Returns: [(project_metadata, score), ...]
        """
        if not query.strip():
            return []
        if self._vector_store is None:
            # ì—°ê²° ì‹œë„ (lazy)
            try:
                self._load_faiss_index()
            except Exception:
                return []

        try:
            results = self._vector_store.similarity_search_with_score(query, k=k)
            # results: List[ (Document, score) ]
            formatted: List[Tuple[Dict[str, Any], float]] = []
            for doc, score in results:
                meta = doc.metadata or {}
                # FAISSì—ì„œëŠ” ê±°ë¦¬ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                # ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ê°€ ë†’ìœ¼ë¯€ë¡œ ì—­ë³€í™˜
                distance = float(score)
                # ê±°ë¦¬ë¥¼ 0-1 ë²”ìœ„ì˜ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ê°€ 0ì´ë©´ ìœ ì‚¬ë„ 1, ê±°ë¦¬ê°€ í´ìˆ˜ë¡ ìœ ì‚¬ë„ ê°ì†Œ)
                similarity_score = 1.0 / (1.0 + distance)
                formatted.append((meta, similarity_score))
            return formatted
        except Exception as e:
            print(f"âŒ FAISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _encode_texts(self, texts: List[str]) -> np.ndarray:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        """
        if not texts:
            return np.empty((0, 0))

        if self._backend == "openai" and self._openai_embeddings is not None:
            vectors: List[List[float]] = self._openai_embeddings.embed_documents(texts)
            return np.array(vectors, dtype=np.float32)

        # OpenAI ì„ë² ë”©ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì•ˆ ë¨
        raise RuntimeError("OpenAI ì„ë² ë”©ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """
        ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            text1 (str): ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
            text2 (str): ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
            
        Returns:
            float: ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not text1 or not text2:
            return 0.0
        
        # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        embeddings = self._encode_texts([text1, text2])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)
    
    def calculate_tag_similarity(self, user_tags: List[str], project_tags: List[str]) -> float:
        """
        íƒœê·¸ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            user_tags (List[str]): ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì¶”ì¶œí•œ íƒœê·¸ë“¤
            project_tags (List[str]): í”„ë¡œì íŠ¸ì˜ íƒœê·¸ë“¤
            
        Returns:
            float: íƒœê·¸ ìœ ì‚¬ë„ ì ìˆ˜ (0.0 ~ 1.0)
        """
        if not user_tags or not project_tags:
            return 0.0
        
        # íƒœê·¸ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚°
        user_tag_text = " ".join(user_tags)
        project_tag_text = " ".join(project_tags)
        
        return self.calculate_similarity(user_tag_text, project_tag_text)
    
    def extract_keywords_from_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ (ê°„ë‹¨í•œ í˜•íƒœì†Œ ë¶„ì„)
        
        Args:
            text (str): í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
            
        Returns:
            List[str]: ì¶”ì¶œëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        if not text:
            return []
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œí•˜ëŠ” ì •ê·œí‘œí˜„ì‹
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text)
        
        # 2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ í•„í„°ë§ (ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œë§Œ ì¶”ì¶œ)
        keywords = [word for word in words if len(word) >= 2]
        
        return keywords
    
    def calculate_project_similarity(self, user_request: str, project: Dict[str, Any]) -> Dict[str, float]:
        """
        ì‚¬ìš©ì ìš”ì²­ê³¼ í”„ë¡œì íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
        
        Args:
            user_request (str): ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ìš”ì²­ ë‚´ìš©
            project (Dict[str, Any]): ë¹„êµí•  í”„ë¡œì íŠ¸ ì •ë³´
            
        Returns:
            Dict[str, float]: ê° ìš”ì†Œë³„ ìœ ì‚¬ë„ ì ìˆ˜
        """
        project_title = project.get('í”„ë¡œì íŠ¸ëª…', '')
        project_overview = project.get('í”„ë¡œì íŠ¸ê°œìš”', '')
        project_tags = project.get('í”„ë¡œì íŠ¸íƒœê·¸', [])
        
        # ê° ìš”ì†Œë³„ ìœ ì‚¬ë„ ê³„ì‚°
        title_similarity = self.calculate_similarity(user_request, project_title)
        overview_similarity = self.calculate_similarity(user_request, project_overview)
        
        # íƒœê·¸ ìœ ì‚¬ë„ ê³„ì‚°
        user_keywords = self.extract_keywords_from_text(user_request)
        tag_similarity = self.calculate_tag_similarity(user_keywords, project_tags)
        
        return {
            'title': title_similarity,
            'overview': overview_similarity,
            'tags': tag_similarity
        }

    def calculate_project_similarity_from_analysis(self, analysis: Dict[str, Any], project: Dict[str, Any]) -> Dict[str, float]:
        """
        Pydantic ë¶„ì„ ê²°ê³¼(íŠ¹ì„±/íƒœê·¸/í•„ìš”ì—­ëŸ‰)ì— ê¸°ë°˜í•œ ìœ ì‚¬ë„ ê³„ì‚°
        Returns keys: request_similarity, content_similarity, tag_similarity
        """
        if not analysis:
            return {'request_similarity': 0.0, 'content_similarity': 0.0, 'tag_similarity': 0.0}

        required_caps: str = analysis.get('required_capabilities', '') or ''
        project_char: str = analysis.get('project_characteristics', '') or ''
        req_tags: List[str] = analysis.get('tags', []) or []

        project_overview = project.get('í”„ë¡œì íŠ¸ê°œìš”', '') or ''
        project_tags: List[str] = project.get('í”„ë¡œì íŠ¸íƒœê·¸', []) or []

        # 1) í•„ìš” ì—­ëŸ‰ vs í”„ë¡œì íŠ¸ ê°œìš”
        request_similarity = self.calculate_similarity(required_caps, project_overview) if required_caps and project_overview else 0.0

        # 2) í”„ë¡œì íŠ¸ íŠ¹ì„± vs í”„ë¡œì íŠ¸ ê°œìš”
        content_similarity = self.calculate_similarity(project_char, project_overview) if project_char and project_overview else 0.0

        # 3) íƒœê·¸ ë¦¬ìŠ¤íŠ¸ ìœ ì‚¬ë„ (ìš”ì²­ íƒœê·¸ë³„ë¡œ í”„ë¡œì íŠ¸ íƒœê·¸ì™€ì˜ ìµœëŒ€ ìœ ì‚¬ë„ì˜ í‰ê· )
        tag_similarity = 0.0
        if req_tags and project_tags:
            per_tag_scores: List[float] = []
            for t in req_tags:
                # ê° ìš”ì²­ íƒœê·¸ tì— ëŒ€í•´ í”„ë¡œì íŠ¸ íƒœê·¸ë“¤ê³¼ì˜ ìµœëŒ€ ìœ ì‚¬ë„
                if not t:
                    continue
                max_sim = 0.0
                for pt in project_tags:
                    if not pt:
                        continue
                    sim = self.calculate_similarity(str(t), str(pt))
                    if sim > max_sim:
                        max_sim = sim
                per_tag_scores.append(max_sim)
            if per_tag_scores:
                tag_similarity = float(sum(per_tag_scores) / len(per_tag_scores))

        return {
            'request_similarity': float(request_similarity),
            'content_similarity': float(content_similarity),
            'tag_similarity': float(tag_similarity)
        }
    
    def calculate_time_weight(self, project: Dict[str, Any]) -> float:
        """
        í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì‹œì ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ
        
        ìµœê·¼ì— ìˆ˜í–‰í•œ í”„ë¡œì íŠ¸ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
        
        Args:
            project (Dict[str, Any]): í”„ë¡œì íŠ¸ ì •ë³´
            
        Returns:
            float: ì‹œê°„ ê°€ì¤‘ì¹˜ (0.8 ~ 1.5)
        """
        try:
            # í”„ë¡œì íŠ¸ ì¢…ë£Œì¼ íŒŒì‹±
            end_date_str = project.get('í”„ë¡œì íŠ¸ê¸°ê°„', '').split(' ~ ')[-1]
            if not end_date_str:
                return 1.0
            
            end_date = datetime.strptime(end_date_str, '%Y.%m')
            current_date = datetime.now()
            
            # ê°œì›” ì°¨ì´ ê³„ì‚°
            months_diff = (current_date.year - end_date.year) * 12 + (current_date.month - end_date.month)
            
            # ìµœê·¼ í”„ë¡œì íŠ¸ì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš©
            if months_diff <= 6:      # 6ê°œì›” ì´ë‚´: 1.5ë°°
                return RECENT_PROJECT_WEIGHT
            elif months_diff <= 12:   # 1ë…„ ì´ë‚´: 1.2ë°°
                return 1.2
            elif months_diff <= 24:   # 2ë…„ ì´ë‚´: 1.0ë°°
                return 1.0
            else:                     # 2ë…„ ì´ìƒ: 0.8ë°°
                return 0.8
                
        except (ValueError, IndexError):
            # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì ìš©
            return 1.0
    
    def calculate_participant_suitability(self, analysis: Dict[str, Any], participant_name: str, 
                                        participant_projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        íŠ¹ì • ì°¸ì—¬ìì˜ ì í•©ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œ (FAISS ê¸°ë°˜)
        
        ì´ ë©”ì„œë“œëŠ” FAISS ë²¡í„° DBë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì°¸ì—¬ìì˜ ì í•©ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            user_request (str): ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ìš”ì²­ ë‚´ìš©
            participant_name (str): ì°¸ì—¬ì ì´ë¦„
            participant_projects (List[Dict[str, Any]]): ì°¸ì—¬ìì˜ í”„ë¡œì íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict[str, Any]: ì°¸ì—¬ìì˜ ì í•©ë„ ë¶„ì„ ê²°ê³¼
        """
        if not participant_projects:
            return {
                'participant': participant_name,
                'total_score': 0.0,
                'project_count': 0,
                'recent_score': 0.0,
                'best_matches': [],
                'reasons': []
            }
        
        # FAISS ë²¡í„° DB í™•ì¸ ë° ìë™ ë™ê¸°í™”
        if not self._check_and_sync_vector_db():
            # ë²¡í„° DB ë™ê¸°í™” ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            return self._calculate_participant_suitability_fallback(analysis, participant_name, participant_projects)
        
            # FAISS ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ (í›„ë³´ ì„ ë³„/ì´ˆê¸° ë­í‚¹ìš©)
        try:
            # ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ ì§ˆì˜ êµ¬ì„±
            query_parts: List[str] = []
            req_caps = (analysis or {}).get('required_capabilities') or ''
            proj_char = (analysis or {}).get('project_characteristics') or ''
            tags = (analysis or {}).get('tags') or []
            if req_caps:
                query_parts.append(req_caps)
            if proj_char:
                query_parts.append(proj_char)
            if tags:
                query_parts.append(', '.join(tags))
            query = '\n'.join(query_parts) if query_parts else ''
            similar_projects = self.search_similar_projects(query, k=100) if query else []
            
            # ì°¸ì—¬ìê°€ ì°¸ì—¬í•œ í”„ë¡œì íŠ¸ ì¤‘ì—ì„œ ë§¤ì¹­ë˜ëŠ” ê²ƒë“¤ í•„í„°ë§
            # í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ë§¤ì¹­ (ë” ì•ˆì •ì )
            participant_project_names = {project.get('í”„ë¡œì íŠ¸ëª…', '') for project in participant_projects}
            matching_projects = []
            
            for meta, score in similar_projects:
                project_name = meta.get('project_name', '')
                if project_name in participant_project_names:
                    # ì›ë³¸ í”„ë¡œì íŠ¸ ë°ì´í„° ì°¾ê¸°
                    for project in participant_projects:
                        if project.get('í”„ë¡œì íŠ¸ëª…', '') == project_name:
                            # ë¶„ì„ ê¸°ë°˜ ìƒì„¸ ìœ ì‚¬ë„ ë° ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚°
                            similarities = self.calculate_project_similarity_from_analysis(analysis, project)
                            time_weight = self.calculate_time_weight(project)
                            # ê°€ì¤‘ì¹˜ ì„¤ì •: request 0.5, content 0.25, tags 0.25
                            project_score = (
                                similarities['request_similarity'] * 0.5 +
                                similarities['content_similarity'] * 0.25 +
                                similarities['tag_similarity'] * 0.25
                            ) * time_weight
                            matching_projects.append({
                                'project': project,
                                'score': project_score,
                                'similarities': similarities,
                                'time_weight': time_weight
                            })
                            break
            
            if not matching_projects:
                return {
                    'participant': participant_name,
                    'total_score': 0.0,
                    'project_count': len(participant_projects),
                    'recent_score': 0.0,
                    'recent_project_count': 0,
                    'best_matches': [],
                    'reasons': ["ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤."]
                }
            
            # ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬ (í´ë°±ê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í‰ê·  ì‚°ì¶œ)
            total_score = sum(match['score'] for match in matching_projects)
            avg_score = total_score / len(matching_projects)
            
            # ìµœê·¼ í”„ë¡œì íŠ¸ ì ìˆ˜ ê³„ì‚°
            recent_score = 0.0
            recent_count = 0
            for match in matching_projects:
                if match['time_weight'] >= RECENT_PROJECT_WEIGHT:
                    recent_score += match['score']
                    recent_count += 1
            
            recent_avg_score = recent_score / recent_count if recent_count > 0 else 0.0
            
            # ìƒìœ„ ë§¤ì¹­ í”„ë¡œì íŠ¸ë“¤ ì •ë ¬
            matching_projects.sort(key=lambda x: x['score'], reverse=True)
            best_matches = matching_projects[:3]
            
            # ì¶”ì²œ ì´ìœ  ìƒì„±
            reasons = self._generate_recommendation_reasons(participant_name, best_matches, recent_avg_score)
            
            return {
                'participant': participant_name,
                'total_score': avg_score,
                'recent_score': recent_avg_score,
                'project_count': len(participant_projects),
                'recent_project_count': recent_count,
                'best_matches': best_matches,
                'reasons': reasons
            }
            
        except Exception as e:
            print(f"âš ï¸ FAISS ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨, í´ë°± ë°©ì‹ ì‚¬ìš©: {e}")
            return self._calculate_participant_suitability_fallback(analysis, participant_name, participant_projects)
    
    def _check_and_sync_vector_db(self) -> bool:
        """
        FAISS ë²¡í„° DBê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ë²¡í„° DBê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        # ë²¡í„° DBê°€ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if self._vector_store is not None:
            return True
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        faiss_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.faiss")
        metadata_path = os.path.join(VECTOR_DB_DIR, f"{VECTOR_COLLECTION_NAME}.pkl")
        
        if os.path.exists(faiss_path) and os.path.exists(metadata_path):
            # íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ ì‹œë„
            try:
                self._load_faiss_index()
                return self._vector_store is not None
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
                return False
        else:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìë™ ë™ê¸°í™” ì‹¤í–‰
            print("ğŸ”„ FAISS ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ìë™ìœ¼ë¡œ ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            try:
                doc_count = self.sync_vector_db()
                if doc_count > 0:
                    print(f"âœ… ë²¡í„° DB ë™ê¸°í™” ì™„ë£Œ: {doc_count}ê°œ ë¬¸ì„œ ì €ì¥")
                    return True
                else:
                    print("âŒ ë²¡í„° DB ë™ê¸°í™” ì‹¤íŒ¨")
                    return False
            except Exception as e:
                print(f"âŒ ë²¡í„° DB ë™ê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return False
    
    def _calculate_participant_suitability_fallback(self, analysis: Dict[str, Any], participant_name: str, 
                                                  participant_projects: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì°¸ì—¬ì ì í•©ë„ë¥¼ ê³„ì‚°í•˜ëŠ” í´ë°± ë©”ì„œë“œ
        
        FAISSê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ ê¸°ì¡´ì˜ ì„ë² ë”© ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        # 1íšŒ í•œì • ì¬ìƒ‰ì¸ ì‹œë„ (ë¬´í•œë£¨í”„ ë°©ì§€ í”Œë˜ê·¸ ì‚¬ìš©)
        try:
            if not getattr(self, "_reindex_attempted", False):
                self._reindex_attempted = True
                # ì¸ë±ìŠ¤ ë™ê¸°í™” ì‹œë„
                doc_count = self.sync_vector_db()
                if doc_count > 0:
                    # ì¬ìƒ‰ì¸ ì„±ê³µ ì‹œ ë™ì¼ ë¡œì§ìœ¼ë¡œ ì¬ì‹œë„
                    try:
                        result = self.calculate_participant_suitability(analysis, participant_name, participant_projects)
                        return result
                    except Exception:
                        # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ í´ë°± ê³„ì† ì§„í–‰
                        pass
        finally:
            # ë‹¤ìŒ í˜¸ì¶œì—ëŠ” ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆë„ë¡ ë¦¬ì…‹
            if hasattr(self, "_reindex_attempted"):
                self._reindex_attempted = False

        if not participant_projects:
            return {
                'participant': participant_name,
                'total_score': 0.0,
                'project_count': 0,
                'recent_score': 0.0,
                'best_matches': [],
                'reasons': []
            }
        
        total_score = 0.0
        project_scores = []
        recent_score = 0.0
        recent_count = 0
        
        # ê° í”„ë¡œì íŠ¸ë³„ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ë¶„ì„ ê¸°ë°˜)
        for project in participant_projects:
            # í”„ë¡œì íŠ¸ë³„ ìœ ì‚¬ë„ ê³„ì‚° (ë¶„ì„ ê¸°ë°˜)
            similarities = self.calculate_project_similarity_from_analysis(analysis, project)
            
            # ì‹œê°„ ê°€ì¤‘ì¹˜ ê³„ì‚°
            time_weight = self.calculate_time_weight(project)
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            project_score = (
                similarities['request_similarity'] * 0.5 +
                similarities['content_similarity'] * 0.25 +
                similarities['tag_similarity'] * 0.25
            ) * time_weight
            
            total_score += project_score
            project_scores.append({
                'project': project,
                'score': project_score,
                'similarities': similarities,
                'time_weight': time_weight
            })
            
            # ìµœê·¼ í”„ë¡œì íŠ¸ ì ìˆ˜ (6ê°œì›” ì´ë‚´)
            if time_weight >= RECENT_PROJECT_WEIGHT:
                recent_score += project_score
                recent_count += 1
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚°
        avg_score = total_score / len(participant_projects) if participant_projects else 0.0
        recent_avg_score = recent_score / recent_count if recent_count > 0 else 0.0
        
        # ìƒìœ„ ë§¤ì¹­ í”„ë¡œì íŠ¸ë“¤ (ì ìˆ˜ ê¸°ì¤€ ì •ë ¬)
        project_scores.sort(key=lambda x: x['score'], reverse=True)
        best_matches = project_scores[:3]  # ìƒìœ„ 3ê°œ
        
        # ì¶”ì²œ ì´ìœ  ìƒì„±
        reasons = self._generate_recommendation_reasons(participant_name, best_matches, recent_avg_score)
        
        return {
            'participant': participant_name,
            'total_score': avg_score,
            'recent_score': recent_avg_score,
            'project_count': len(participant_projects),
            'recent_project_count': recent_count,
            'best_matches': best_matches,
            'reasons': reasons
        }

    def _generate_recommendation_reasons(self, participant_name: str, best_matches: List[Dict], 
                                       recent_score: float) -> List[str]:
        """
        ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„±í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ” ì°¸ì—¬ìì˜ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë¶„ì„í•˜ì—¬
        ì™œ í•´ë‹¹ ì¸ì›ì´ ì í•©í•œì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì´ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            participant_name (str): ì°¸ì—¬ì ì´ë¦„
            best_matches (List[Dict]): ìƒìœ„ ë§¤ì¹­ í”„ë¡œì íŠ¸ë“¤
            recent_score (float): ìµœê·¼ í”„ë¡œì íŠ¸ ì ìˆ˜
            
        Returns:
            List[str]: ì¶”ì²œ ì´ìœ  ë¦¬ìŠ¤íŠ¸
        """
        reasons = []
        
        if not best_matches:
            return ["ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì—†ìŠµë‹ˆë‹¤."]
        
        # 1. ìµœê³  ë§¤ì¹­ í”„ë¡œì íŠ¸ ê¸°ë°˜ ìƒì„¸ ì´ìœ 
        best_match = best_matches[0]
        project = best_match['project']
        score = best_match['score']
        similarities = best_match['similarities']
        
        # í”„ë¡œì íŠ¸ëª… ê´€ë ¨ì„± ë¶„ì„
        if similarities['title'] > 0.6:
            reasons.append(f"ğŸ¯ **í”„ë¡œì íŠ¸ëª… ìœ ì‚¬ë„ ë†’ìŒ**: '{project['í”„ë¡œì íŠ¸ëª…']}' í”„ë¡œì íŠ¸ì™€ ì§ì ‘ì ì¸ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        elif similarities['title'] > 0.4:
            reasons.append(f"ğŸ“‹ **í”„ë¡œì íŠ¸ëª… ë¶€ë¶„ ë§¤ì¹­**: '{project['í”„ë¡œì íŠ¸ëª…']}' í”„ë¡œì íŠ¸ì™€ ì¼ë¶€ ê´€ë ¨ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        
        # í”„ë¡œì íŠ¸ ê°œìš” ê´€ë ¨ì„± ë¶„ì„
        if similarities['overview'] > 0.7:
            reasons.append(f"ğŸ“ **í”„ë¡œì íŠ¸ ë‚´ìš© ë†’ì€ ì¼ì¹˜**: '{project['í”„ë¡œì íŠ¸ê°œìš”'][:50]}...' ë‚´ìš©ê³¼ ë§¤ìš° ìœ ì‚¬í•œ ê²½í—˜ì„ ë³´ìœ í•©ë‹ˆë‹¤.")
        elif similarities['overview'] > 0.5:
            reasons.append(f"ğŸ“„ **í”„ë¡œì íŠ¸ ë‚´ìš© ë¶€ë¶„ ì¼ì¹˜**: '{project['í”„ë¡œì íŠ¸ê°œìš”'][:50]}...' ë‚´ìš©ê³¼ ê´€ë ¨ëœ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
        
        # íƒœê·¸ ë§¤ì¹­ ìƒì„¸ ë¶„ì„
        if similarities['tags'] > 0.6:
            project_tags = project.get('í”„ë¡œì íŠ¸íƒœê·¸', [])
            if project_tags:
                reasons.append(f"ğŸ·ï¸ **ì „ë¬¸ ë¶„ì•¼ ì¼ì¹˜**: '{', '.join(project_tags[:4])}' ë¶„ì•¼ì—ì„œ ì „ë¬¸ì„±ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        elif similarities['tags'] > 0.4:
            project_tags = project.get('í”„ë¡œì íŠ¸íƒœê·¸', [])
            if project_tags:
                reasons.append(f"ğŸ”– **ê´€ë ¨ ë¶„ì•¼ ê²½í—˜**: '{', '.join(project_tags[:3])}' ë¶„ì•¼ì™€ ê´€ë ¨ëœ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
        
        # 2. í”„ë¡œì íŠ¸ ê¸°ê°„ ë° ìµœê·¼ì„± ë¶„ì„
        project_period = project.get('í”„ë¡œì íŠ¸ê¸°ê°„', '')
        if project_period:
            try:
                end_date_str = project_period.split(' ~ ')[-1]
                if end_date_str:
                    end_date = datetime.strptime(end_date_str, '%Y.%m')
                    current_date = datetime.now()
                    months_diff = (current_date.year - end_date.year) * 12 + (current_date.month - end_date.month)
                    
                    if months_diff <= 6:
                        reasons.append(f"â° **ìµœê·¼ ê²½í—˜**: {project_period}ì— ì™„ë£Œëœ ìµœê·¼ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë³´ìœ í•©ë‹ˆë‹¤.")
                    elif months_diff <= 12:
                        reasons.append(f"ğŸ“… **1ë…„ ì´ë‚´ ê²½í—˜**: {project_period}ì— ì™„ë£Œëœ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë³´ìœ í•©ë‹ˆë‹¤.")
                    else:
                        reasons.append(f"ğŸ“† **ê³¼ê±° ê²½í—˜**: {project_period}ì— ì™„ë£Œëœ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë³´ìœ í•©ë‹ˆë‹¤.")
            except:
                reasons.append(f"ğŸ“… **í”„ë¡œì íŠ¸ ê¸°ê°„**: {project_period}ì— ì°¸ì—¬í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
        
        # 3. ë‹¤ì¤‘ í”„ë¡œì íŠ¸ ê²½í—˜ ë¶„ì„
        if len(best_matches) >= 2:
            second_match = best_matches[1]
            second_project = second_match['project']
            reasons.append(f"ğŸ”„ **ë‹¤ì–‘í•œ ê´€ë ¨ ê²½í—˜**: '{second_project['í”„ë¡œì íŠ¸ëª…']}' ë“± ì´ {len(best_matches)}ê°œì˜ ê´€ë ¨ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.")
        
        # 4. ìµœê·¼ í™œë™ë„ ë¶„ì„
        if recent_score > 0.6:
            reasons.append("ğŸš€ **ë†’ì€ ìµœê·¼ í™œë™ë„**: ìµœê·¼ 6ê°œì›” ë‚´ ê´€ë ¨ í”„ë¡œì íŠ¸ì—ì„œ ë›°ì–´ë‚œ ì„±ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.")
        elif recent_score > 0.4:
            reasons.append("ğŸ“ˆ **í™œë°œí•œ ìµœê·¼ í™œë™**: ìµœê·¼ 1ë…„ ë‚´ ê´€ë ¨ í”„ë¡œì íŠ¸ì— í™œë°œíˆ ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤.")
        
        # 5. ì „ì²´ í”„ë¡œì íŠ¸ ì°¸ì—¬ ê²½í—˜
        total_projects = len([m for m in best_matches if m['score'] > 0.3])
        if total_projects >= 5:
            reasons.append(f"ğŸ’¼ **í’ë¶€í•œ í”„ë¡œì íŠ¸ ê²½í—˜**: ì´ {total_projects}ê°œ ì´ìƒì˜ ê´€ë ¨ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í•œ í’ë¶€í•œ ê²½í—˜ì„ ë³´ìœ í•©ë‹ˆë‹¤.")
        elif total_projects >= 3:
            reasons.append(f"ğŸ¯ **ì¶©ë¶„í•œ í”„ë¡œì íŠ¸ ê²½í—˜**: {total_projects}ê°œì˜ ê´€ë ¨ í”„ë¡œì íŠ¸ì— ì°¸ì—¬í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.")
        
        # 6. ì ìˆ˜ ê¸°ë°˜ ì¢…í•© í‰ê°€
        if score > 0.8:
            reasons.append("â­ **ë§¤ìš° ë†’ì€ ì í•©ë„**: ìš”ì²­í•˜ì‹  í”„ë¡œì íŠ¸ì™€ ë§¤ìš° ë†’ì€ ê´€ë ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        elif score > 0.6:
            reasons.append("âœ… **ë†’ì€ ì í•©ë„**: ìš”ì²­í•˜ì‹  í”„ë¡œì íŠ¸ì™€ ë†’ì€ ê´€ë ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        elif score > 0.4:
            reasons.append("ğŸ‘ **ì ì ˆí•œ ì í•©ë„**: ìš”ì²­í•˜ì‹  í”„ë¡œì íŠ¸ì™€ ì ì ˆí•œ ê´€ë ¨ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        
        # 7. íŠ¹ë³„í•œ ê°•ì ì´ ìˆëŠ” ê²½ìš°
        if similarities['title'] > 0.7 and similarities['overview'] > 0.7:
            reasons.append("ğŸ–ï¸ **ì „ë¬¸ì„± ì¸ì¦**: í”„ë¡œì íŠ¸ëª…ê³¼ ë‚´ìš© ëª¨ë‘ì—ì„œ ë†’ì€ ì¼ì¹˜ë„ë¥¼ ë³´ì´ëŠ” ì „ë¬¸ì„±ì„ ì¸ì •ë°›ìŠµë‹ˆë‹¤.")
        
        return reasons
